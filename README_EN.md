<h1 align='center'>🌸 DaiyuLM: Lin Daiyu-style Emotional Dialogue Model</h1>
<p align="center">
  <img src="assets/daiyu.png" alt="Daiyu" width="140"/>
</p>

<h3 align="center">
A fine-tuned Chinese model based on ChatGLM2-6B, adept at generating sentimental, subtle, and slightly resentful responses in the literary style of "Lin Daiyu".
</h3>

<p align="center">
  <a href="README.md">简体中文</a> | <b>English</b>
</p>

<p align="center">
  <a href="https://huggingface.co/spaces/caixiaoshun/DaiyuLM">Live Demo</a> · 
  <a href="https://github.com/caixiaoshun/DaiyuLM">GitHub</a> · 
  <a href="mailto:cs.shunzhang@foxmail.com">Contact</a>
</p>

---

## 🌟 Highlights

- 🎭 Fine-tuned on 142 dialogues in the "Lin Daiyu" style — poetic, emotional, implicit and dramatic
- ⚙️ Built upon ChatGLM2-6B using Prefix-Tuning, updating only a small set of parameters for efficient deployment
- 💻 Interactive Gradio interface with real-time streaming responses

---

## 📚 Dataset Sources

- 🔍 Total of 142 Chinese QA pairs:
  - 21 carefully selected from Douban, Zhihu, etc., exemplifying authentic Lin Daiyu-style expressions
  - 121 generated by GPT-4o and manually refined to ensure consistency in style
- Topics include "emotional breakdown", "misunderstanding", and "love", emphasizing subtle resentment and delicate expression

---

## 🚀 Quick Start

### 1️⃣ Install Dependencies

```bash
git clone https://github.com/caixiaoshun/DaiyuLM.git
cd DaiyuLM
pip install -r requirements.txt
````

### 2️⃣ Prepare the Model

* Download the [ChatGLM2‑6B](https://huggingface.co/THUDM/chatglm2-6b) base model and tokenizer.
  The simplest way is via the Hugging Face CLI:

```bash
huggingface-cli download --resume-download THUDM/chatglm2-6b --local-dir checkpoint/chatglm2-6b
```

* For users in China who experience slow downloads, set the following mirror before downloading:

```bash
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download --resume-download THUDM/chatglm2-6b --local-dir checkpoint/chatglm2-6b
```

* Place the fine-tuned prefix parameters into the `checkpoint/` folder:

```bash
DaiyuLM/
├── app.py
├── requirements.txt
├── README.md
├── README_EN.md
├── checkpoint/
│   ├── chatglm2-6b/                         # downloaded base model
│   ├── pytorch_model.bin                   # trained on 21 online samples (epoch not recorded)
│   └── pytorch_model_step_253000_141.bin   # trained on 142 samples, saved at step 253000
```

You can then launch the Gradio app with:

```bash
python app.py --model_name_or_path checkpoint/chatglm2-6b --checkpoint checkpoint/pytorch_model_step_253000_141.bin
```

If no arguments are passed, the app defaults to `checkpoint/pytorch_model.bin`.


---

## 💬 Dialogue Examples

![lindaiyu](https://github.com/user-attachments/assets/2b4b5c0f-f9ff-4a94-a68e-7d2841c4ba65)

| User Input                   | Lin Daiyu-style Response                                                                                               |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Why haven’t you replied yet? | I fear I won’t last through the day — just waiting for your message is already exhausting.                             |
| Are you ignoring me?         | If this is your attitude, brother, perhaps ignoring me altogether would spare me the accusation of being unreasonable. |
| It was just a joke...        | But I am not someone to be joked about. Your casual words haunted me the entire night.                                 |

---

## 🧠 Model Details

* **Base Model**: ChatGLM2-6B
* **Fine-tuning Method**: Prefix-Tuning (prefix length = 128)
* **Streaming Support**: ✅
* **Parameter Change**: Only \~0.1% of parameters updated — full finetuning not required

---

## 🔧 Future Directions

* 🔁 Extend training to cover more characters from *Dream of the Red Chamber*, such as Jia Baoyu and Wang Xifeng
* 🧪 Add API deployment support for web, WeChat Mini Program, or QQ Bots
* 🖥️ Enable INT4 quantization for low-resource environments

---

## 🙋 FAQ

**Q1: Can I run this without a GPU?**
Yes, inference works on CPU, though it may be slower.

**Q2: How can I use INT4 for acceleration?**
We recommend using the `chatglm2-6b-int4` variant, or loading with `bitsandbytes` for quantized inference.

---

## 📬 Contact

Want to contribute styles or deploy this as an API? Reach out or file an issue.

* 📮 Email: `cs.shunzhang@foxmail.com`
* 🧠 Inspired by the character Lin Daiyu in *Dream of the Red Chamber* and the rise of emotional "neurotic" literature online

---

## ✅ Support the Author

If you enjoy this project, feel free to support the author with a cup of tea ☕:

<p align="center">
  <img src="assets/wechat_pay.png" alt="WeChat Pay" height="260" style="margin-right: 20px;"/>
  <img src="assets/alipay.png" alt="Alipay" height="260"/>
</p>

<p align="center"><em>Scan to support❤️</em></p>

---

## 📄 License

This project is released under the [Apache 2.0 License](LICENSE). Commercial use, modification, and redistribution are welcome.

---

> 💡 **Acknowledgements**: Thanks to the THUDM team for open-sourcing ChatGLM2, and to Lin Daiyu — a timeless muse for poetic melancholy.

