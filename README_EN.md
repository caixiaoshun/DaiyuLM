<h1 align='center'>ğŸŒ¸ DaiyuLM: Lin Daiyu-style Emotional Dialogue Model</h1>
<p align="center">
  <img src="assets/daiyu.png" alt="Daiyu" width="140"/>
</p>

<h3 align="center">
A fine-tuned Chinese model based on ChatGLM2-6B, adept at generating sentimental, subtle, and slightly resentful responses in the literary style of "Lin Daiyu".
</h3>

<p align="center">
  <a href="README.md">ç®€ä½“ä¸­æ–‡</a> | <b>English</b>
</p>

<p align="center">
  <a href="https://huggingface.co/spaces/caixiaoshun/DaiyuLM">Live Demo</a> Â· 
  <a href="https://github.com/caixiaoshun/DaiyuLM">GitHub</a> Â· 
  <a href="mailto:cs.shunzhang@foxmail.com">Contact</a>
</p>

---

## ğŸŒŸ Highlights

- ğŸ­ Fine-tuned on 142 dialogues in the "Lin Daiyu" style â€” poetic, emotional, implicit and dramatic
- âš™ï¸ Built upon ChatGLM2-6B using Prefix-Tuning, updating only a small set of parameters for efficient deployment
- ğŸ’» Interactive Gradio interface with real-time streaming responses

---

## ğŸ“š Dataset Sources

- ğŸ” Total of 142 Chinese QA pairs:
  - 21 carefully selected from Douban, Zhihu, etc., exemplifying authentic Lin Daiyu-style expressions
  - 121 generated by GPT-4o and manually refined to ensure consistency in style
- Topics include "emotional breakdown", "misunderstanding", and "love", emphasizing subtle resentment and delicate expression

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Dependencies

```bash
git clone https://github.com/caixiaoshun/DaiyuLM.git
cd DaiyuLM
pip install -r requirements.txt
````

### 2ï¸âƒ£ Prepare the Model

* Download the [ChatGLM2â€‘6B](https://huggingface.co/THUDM/chatglm2-6b) base model and tokenizer.
  The simplest way is via the Hugging Face CLI:

```bash
huggingface-cli download --resume-download THUDM/chatglm2-6b --local-dir checkpoint/chatglm2-6b
```

* For users in China who experience slow downloads, set the following mirror before downloading:

```bash
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download --resume-download THUDM/chatglm2-6b --local-dir checkpoint/chatglm2-6b
```

* Place the fine-tuned prefix parameters into the `checkpoint/` folder:

```bash
DaiyuLM/
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ README_EN.md
â”œâ”€â”€ checkpoint/
â”‚   â”œâ”€â”€ chatglm2-6b/                         # downloaded base model
â”‚   â”œâ”€â”€ pytorch_model.bin                   # trained on 21 online samples (epoch not recorded)
â”‚   â””â”€â”€ pytorch_model_step_253000_141.bin   # trained on 142 samples, saved at step 253000
```

You can then launch the Gradio app with:

```bash
python app.py --model_name_or_path checkpoint/chatglm2-6b --checkpoint checkpoint/pytorch_model_step_253000_141.bin
```

If no arguments are passed, the app defaults to `checkpoint/pytorch_model.bin`.


---

## ğŸ’¬ Dialogue Examples

![lindaiyu](https://github.com/user-attachments/assets/2b4b5c0f-f9ff-4a94-a68e-7d2841c4ba65)

| User Input                   | Lin Daiyu-style Response                                                                                               |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Why havenâ€™t you replied yet? | I fear I wonâ€™t last through the day â€” just waiting for your message is already exhausting.                             |
| Are you ignoring me?         | If this is your attitude, brother, perhaps ignoring me altogether would spare me the accusation of being unreasonable. |
| It was just a joke...        | But I am not someone to be joked about. Your casual words haunted me the entire night.                                 |

---

## ğŸ§  Model Details

* **Base Model**: ChatGLM2-6B
* **Fine-tuning Method**: Prefix-Tuning (prefix length = 128)
* **Streaming Support**: âœ…
* **Parameter Change**: Only \~0.1% of parameters updated â€” full finetuning not required

---

## ğŸ”§ Future Directions

* ğŸ” Extend training to cover more characters from *Dream of the Red Chamber*, such as Jia Baoyu and Wang Xifeng
* ğŸ§ª Add API deployment support for web, WeChat Mini Program, or QQ Bots
* ğŸ–¥ï¸ Enable INT4 quantization for low-resource environments

---

## ğŸ™‹ FAQ

**Q1: Can I run this without a GPU?**
Yes, inference works on CPU, though it may be slower.

**Q2: How can I use INT4 for acceleration?**
We recommend using the `chatglm2-6b-int4` variant, or loading with `bitsandbytes` for quantized inference.

---

## ğŸ“¬ Contact

Want to contribute styles or deploy this as an API? Reach out or file an issue.

* ğŸ“® Email: `cs.shunzhang@foxmail.com`
* ğŸ§  Inspired by the character Lin Daiyu in *Dream of the Red Chamber* and the rise of emotional "neurotic" literature online

---

## âœ… Support the Author

If you enjoy this project, feel free to support the author with a cup of tea â˜•:

<p align="center">
  <img src="assets/wechat_pay.png" alt="WeChat Pay" height="260" style="margin-right: 20px;"/>
  <img src="assets/alipay.png" alt="Alipay" height="260"/>
</p>

<p align="center"><em>Scan to supportâ¤ï¸</em></p>

---

## ğŸ“„ License

This project is released under the [Apache 2.0 License](LICENSE). Commercial use, modification, and redistribution are welcome.

---

> ğŸ’¡ **Acknowledgements**: Thanks to the THUDM team for open-sourcing ChatGLM2, and to Lin Daiyu â€” a timeless muse for poetic melancholy.

